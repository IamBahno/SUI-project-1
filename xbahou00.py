# -*- coding: utf-8 -*-
"""zadani.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1j2Q5jZeCPFlwOaiBSwXnY1QfybIWozkt

Vítejte u prvního projektu do SUI.
V rámci projektu Vás čeká několik cvičení, v nichž budete doplňovat poměrně malé fragmenty kódu (místo je vyznačeno pomocí `None` nebo `pass`).
Pokud se v buňce s kódem již něco nachází, využijte/neničte to.
Buňky nerušte ani nepřidávejte.
Snažte se programovat hezky, ale jediná skutečně aktivně zakázaná, vyhledávaná a -- i opakovaně -- postihovaná technika je cyklení přes data (ať už explicitním cyklem nebo v rámci `list`/`dict` comprehension), tomu se vyhýbejte jako čert kříží a řešte to pomocí vhodných operací lineární algebry.

Až budete s řešením hotovi, vyexportujte ho ("Download as") jako PDF i pythonovský skript a ty odevzdejte **pojmenované názvem týmu** (tj. loginem vedoucího).
Dbejte, aby bylo v PDF všechno vidět (nezůstal kód za okrajem stránky apod.).

U všech cvičení je uveden orientační počet řádků řešení.
Berte ho prosím opravdu jako orientační, pozornost mu věnujte, pouze pokud ho významně překračujete.
"""

import numpy as np
import copy
import matplotlib.pyplot as plt
import scipy.stats

"""# Přípravné práce
Prvním úkolem v tomto projektu je načíst data, s nimiž budete pracovat.
Vybudujte jednoduchou třídu, která se umí zkonstruovat z cesty k negativním a pozitivním příkladům, a bude poskytovat:
- pozitivní a negativní příklady (`dataset.pos`, `dataset.neg` o rozměrech [N, 7])
- všechny příklady a odpovídající třídy (`dataset.xs` o rozměru [N, 7], `dataset.targets` o rozměru [N])

K načítání dat doporučujeme využít `np.loadtxt()`.
Netrapte se se zapouzdřováním a gettery, berte třídu jako Plain Old Data.

Načtěte trénovací (`{positives,negatives}.trn`), validační (`{positives,negatives}.val`) a testovací (`{positives,negatives}.tst`) dataset, pojmenujte je po řadě `train_dataset`, `val_dataset` a `test_dataset`.

**(6 řádků)**

"""

class BinaryDataset:
    def __init__(self,pos_dataset_path : str,neg_dataset_path : str) -> None:
        POS = 1
        NEG = 0
        self.pos = np.loadtxt(pos_dataset_path)
        self.neg = np.loadtxt(neg_dataset_path)
        self.xs = np.concatenate((self.pos,self.neg),axis=0)
        self.targets = np.array([POS] * self.pos.shape[0] + [NEG] * self.neg.shape[0])

train_dataset = BinaryDataset('positives.trn', 'negatives.trn')
val_dataset = BinaryDataset('positives.val', 'negatives.val')
test_dataset = BinaryDataset('positives.tst', 'negatives.tst')

print('positives', train_dataset.pos.shape)
print('negatives', train_dataset.neg.shape)
print('xs', train_dataset.xs.shape)
print('targets', train_dataset.targets.shape)

"""V řadě následujících cvičení budete pracovat s jedním konkrétním příznakem. Naimplementujte proto funkci, která vykreslí histogram rozložení pozitivních a negativních příkladů z jedné sady. Nezapomeňte na legendu, ať je v grafu jasné, které jsou které. Funkci zavoláte dvakrát, vykreslete histogram příznaku `5` -- tzn. šestého ze sedmi -- pro trénovací a validační data

**(5 řádků)**
"""

FOI = 5  # Feature Of Interest

def plot_data(poss, negs):
    BINS = 10
    plt.hist(poss,bins=BINS,color='blue',alpha=0.5,label="Positives",histtype="step")
    plt.hist(negs,bins=BINS,color='red',alpha=0.5,label="Negatives",histtype="step")
    plt.xlabel('Value of FOI')
    plt.ylabel('Frequency')
    plt.title(f'Histogram for Feature {FOI}')
    plt.legend()
    plt.tight_layout()
    plt.show()

plot_data(train_dataset.pos[:, FOI], train_dataset.neg[:, FOI])
plot_data(val_dataset.pos[:, FOI], val_dataset.neg[:, FOI])

"""### Evaluace klasifikátorů
Než přistoupíte k tvorbě jednotlivých klasifikátorů, vytvořte funkci pro jejich vyhodnocování.
Nechť se jmenuje `evaluate` a přijímá po řadě klasifikátor, pole dat (o rozměrech [N, F]) a pole tříd ([N]).
Jejím výstupem bude _přesnost_ (accuracy), tzn. podíl správně klasifikovaných příkladů.

Předpokládejte, že klasifikátor poskytuje metodu `.prob_class_1(data)`, která vrací pole posteriorních pravděpodobností třídy 1 pro daná data.
Evaluační funkce bude muset provést tvrdé prahování (na hodnotě 0.5) těchto pravděpodobností a srovnání získaných rozhodnutí s referenčními třídami.
Využijte fakt, že `numpy`ovská pole lze mj. porovnávat se skalárem.

**(3 řádky)**
"""

def evaluate(classifier, inputs, targets):
    probs = classifier.prob_class_1(inputs)
    # look if values are above 0.5 and cast the bolean to int
    predictions = (probs > 0.5).astype(int)
    acc = np.mean(predictions == targets)
    return acc


class Dummy:
    def prob_class_1(self, xs):
        return np.asarray([0.2, 0.7, 0.7])

print(evaluate(Dummy(), None, np.asarray([0, 0, 1])))  # should be 0.66

"""### Baseline
Vytvořte klasifikátor, který ignoruje vstupní data.
Jenom v konstruktoru dostane třídu, kterou má dávat jako tip pro libovolný vstup.
Nezapomeňte, že jeho metoda `.prob_class_1(data)` musí vracet pole správné velikosti.

**(4 řádky)**
"""

class PriorClassifier:
    def __init__(self,_class):
        self._class = _class

    def prob_class_1(self, xs: np.ndarray) -> np.ndarray:
        return np.full(xs.shape[0], self._class)

baseline = PriorClassifier(0)
val_acc = evaluate(baseline, val_dataset.xs[:, FOI], val_dataset.targets)
print('Baseline val acc:', val_acc)

"""# Generativní klasifikátory
V této  části vytvoříte dva generativní klasifikátory, oba založené na Gaussovu rozložení pravděpodobnosti.

Začněte implementací funce, která pro daná 1-D data vrátí Maximum Likelihood odhad střední hodnoty a směrodatné odchylky Gaussova rozložení, které data modeluje.
Funkci využijte pro natrénovaní dvou modelů: pozitivních a negativních příkladů.
Získané parametry -- tzn. střední hodnoty a směrodatné odchylky -- vypíšete.

**(1 řádek)**
"""

def mle_gauss_1d(data):
    return np.mean(data),np.std(data)

mu_pos, std_pos = mle_gauss_1d(train_dataset.pos[:, FOI])
mu_neg, std_neg = mle_gauss_1d(train_dataset.neg[:, FOI])

print('Pos mean: {:.2f} std: {:.2f}'.format(mu_pos, std_pos))
print('Neg mean: {:.2f} std: {:.2f}'.format(mu_neg, std_neg))

"""Ze získaných parametrů vytvořte `scipy`ovská gaussovská rozložení `scipy.stats.norm`.
S využitím jejich metody `.pdf()` vytvořte graf, v němž srovnáte skutečné a modelové rozložení pozitivních a negativních příkladů.
Rozsah x-ové osy volte od -0.5 do 1.5 (využijte `np.linspace`) a u volání `plt.hist()` nezapomeňte nastavit `density=True`, aby byl histogram normalizovaný a dal se srovnávat s modelem.

**(2 + 8 řádků)**
"""

pos_distr = scipy.stats.norm(loc=mu_pos,scale=std_pos)
neg_distr = scipy.stats.norm(loc=mu_neg,scale=std_neg)

BINS = 10
bars = np.linspace(-0.5,1.5,100)
pos_pred = pos_distr.pdf(bars)
neg_pred = neg_distr.pdf(bars)
plt.plot(bars,pos_pred,color='blue',label='Positives (Model)')
plt.plot(bars,neg_pred,color='red',label='Negatives (Model)')
plt.hist((train_dataset.pos[:, FOI], train_dataset.neg[:, FOI]), color=('blue','red'),label=('Positives (Data)','Negatives (Data)'),bins=BINS,density=True,histtype='stepfilled',alpha=0.4)
plt.xlabel(f'Feature {FOI}')
plt.ylabel('Probability density')
plt.legend()
plt.show()

"""Naimplementujte binární generativní klasifikátor.
Při konstrukci přijímá dvě rozložení poskytující metodu `.pdf()` a odpovídající apriorní pravděpodobnost tříd.
Dbejte, aby Vám uživatel nemohl zadat neplatné apriorní pravděpodobnosti.
Jako všechny klasifikátory v tomto projektu poskytuje metodu `prob_class_1()`.

**(9 řádků)**
"""

class GenerativeClassifier2Class:
    def __init__(self, dist_class_0, dist_class_1, prior_class_0, prior_class_1):
        if not (0 <= prior_class_0 <= 1) or not (0 <= prior_class_1 <= 1):
            raise ValueError("Apriorní pravděpodobnosti musí být v rozmezí [0, 1].")
        if not np.isclose(prior_class_0 + prior_class_1, 1.0):
            raise ValueError("Součet apriorních pravděpodobností musí být 1.")

        self.dist_class_0 = dist_class_0
        self.dist_class_1 = dist_class_1
        self.prior_class_0 = prior_class_0
        self.prior_class_1 = prior_class_1

    def prob_class_1(self, data):
        # Výpočet pravděpodobností na základě rozložení a apriorních pravděpodobností
        likelihood_class_0 = self.dist_class_0.pdf(data)
        likelihood_class_1 = self.dist_class_1.pdf(data)

        # Výpočet posteriorních pravděpodobností třídy 1
        numerator = likelihood_class_1 * self.prior_class_1
        denominator = (likelihood_class_0 * self.prior_class_0) + numerator

        # Přechází dělění nulou
        epsilon = 1e-10
        denominator = np.maximum(denominator, epsilon)

        return numerator / denominator

"""Nainstancujte dva generativní klasifikátory: jeden s rovnoměrnými priory a jeden s apriorní pravděpodobností 0.75 pro třídu 0 (negativní příklady).
Pomocí funkce `evaluate()` vyhodnotíte jejich úspěšnost na validačních datech.

**(2 řádky)**
"""

classifier_flat_prior = GenerativeClassifier2Class(neg_distr, pos_distr, 0.5, 0.5)
classifier_full_prior = GenerativeClassifier2Class(neg_distr, pos_distr, 0.75, 0.25)

print('flat:', evaluate(classifier_flat_prior, val_dataset.xs[:, FOI], val_dataset.targets))
print('full:', evaluate(classifier_full_prior, val_dataset.xs[:, FOI], val_dataset.targets))

"""Vykreslete průběh posteriorní pravděpodobnosti třídy 1 jako funkci příznaku 5, opět v rozsahu <-0.5; 1.5> pro oba klasifikátory.
Do grafu zakreslete i histogramy rozložení trénovacích dat, opět s `density=True` pro zachování dynamického rozsahu.

**(8 řádků)**
"""

# Definování rozsahu pro zobrazení
bars = np.linspace(-0.5, 1.5, 100)

posterior_flat = classifier_flat_prior.prob_class_1(bars)
posterior_full = classifier_full_prior.prob_class_1(bars)

plt.figure(figsize=(10, 6))

plt.hist(train_dataset.pos[:, FOI], bins=10, color='blue', alpha=0.4, label='Positives (Data)', density=True, histtype='stepfilled')
plt.hist(train_dataset.neg[:, FOI], bins=10, color='red', alpha=0.4, label='Negatives (Data)', density=True, histtype='stepfilled')

plt.plot(bars, posterior_flat, color='blue', label='Posterior (Flat Prior)', linestyle='--', linewidth=2)
plt.plot(bars, posterior_full, color='orange', label='Posterior (Full Prior)', linestyle='-.', linewidth=2)

plt.xlabel(f'Feature {FOI}')
plt.ylabel('Density / Posterior Probability')
plt.title(f'Posterior Probability of Class 1 as a Function of Feature {FOI}')
plt.legend()
plt.tight_layout()
plt.xlim(-0.5, 1.5)  # Nastavení limitů osy X pro lepší zobrazení
plt.ylim(0, 5.5)  # Nastavení limitů osy Y
plt.show()

"""# Diskriminativní klasifikátory
V následující části budete pomocí (lineární) logistické regrese přímo modelovat posteriorní pravděpodobnost třídy 1.
Modely budou založeny čistě na NumPy, takže nemusíte instalovat nic dalšího.
Nabitějších toolkitů se dočkáte ve třetím projektu.
"""

def logistic_sigmoid(x):
    return np.exp(-np.logaddexp(0, -x))

def binary_cross_entropy(probs, targets):
    return np.sum(-targets * np.log(probs) - (1-targets)*np.log(1-probs))

class LogisticRegressionNumpy:
    def __init__(self, dim):
        self.w = np.array([0.0] * dim)
        self.b = np.array([0.0])

    def prob_class_1(self, x):
        return logistic_sigmoid(x @ self.w + self.b)

"""Diskriminativní klasifikátor očekává, že dostane vstup ve tvaru `[N, F]`.
Pro práci na jediném příznaku bude tedy zapotřebí vyřezávat příslušná data v správném formátu (`[N, 1]`).
Doimplementujte třídu `FeatureCutter` tak, aby to zařizovalo volání její instance.
Který příznak se použije, nechť je konfigurováno při konstrukci.

Může se Vám hodit `np.newaxis`.

**(2 řádky)**
"""

class FeatureCutter:
    def __init__(self, fea_id):
        self.fea_id = fea_id

    def __call__(self, x):
        return x[:, self.fea_id][:, np.newaxis]

"""Dalším krokem je implementovat funkci, která model vytvoří a natrénuje.
Jejím výstupem bude (1) natrénovaný model, (2) průběh trénovací loss a (3) průběh validační přesnosti.
Neuvažujte žádné minibatche, aktualizujte váhy vždy na celém trénovacím datasetu.
Po každém kroku vyhodnoťte model na validačních datech.
Jako model vracejte ten, který dosáhne nejlepší validační přesnosti.
Jako loss použijte binární cross-entropii  a logujte průměr na vzorek.
Pro výpočet validační přesnosti využijte funkci `evaluate()`.
Oba průběhy vracejte jako obyčejné seznamy.

**(cca 11 řádků)**
"""

import copy

def train_logistic_regression(nb_epochs, lr, in_dim, fea_preprocessor):
    model = LogisticRegressionNumpy(in_dim)
    best_model = copy.deepcopy(model)
    losses, accuracies = [], []
    best_accuracy = 0
    train_X = fea_preprocessor(train_dataset.xs)
    train_t = train_dataset.targets

    for _ in range(nb_epochs):
        # Výpočet posteriorních pravděpodobností třídy 1
        probs = model.prob_class_1(train_X)

        # Výpočet ztráty a logování průměrné ztráty na vzorek
        loss = binary_cross_entropy(probs, train_t) / len(train_t)
        losses.append(loss)

        # Výpočet gradientů pro w a b
        grad_w = (train_X.T @ (probs - train_t)) / len(train_t)
        grad_b = np.mean(probs - train_t)
        # Aktualizace vah
        model.w -= lr * grad_w
        model.b -= lr * grad_b
        # Vyhodnocení modelu na validačních datech
        val_X = fea_preprocessor(val_dataset.xs)
        val_acc = evaluate(model, val_X, val_dataset.targets)
        accuracies.append(val_acc)

        if val_acc > best_accuracy:
            best_accuracy = val_acc
            best_model = copy.deepcopy(model)

    return best_model, losses, accuracies

"""Funkci zavolejte a natrénujte model.
Uveďte zde parametry, které vám dají slušný výsledek.
Měli byste dostat přesnost srovnatelnou s generativním klasifikátorem s nastavenými priory.
Neměli byste potřebovat víc, než 100 epoch.
Vykreslete průběh trénovací loss a validační přesnosti, osu x značte v epochách.

V druhém grafu vykreslete histogramy trénovacích dat a pravděpodobnost třídy 1 pro x od -0.5 do 1.5, podobně jako výše u generativních klasifikátorů.

**(1 + 5 + 8 řádků)**
"""

nb_epochs = 100  # Maximální počet epoch
learning_rate = 5  # Učební koeficient
in_dim = 1  # Počet vstupních dimenzí (jeden příznak)

fea_cutter = FeatureCutter(FOI)
disc_fea5, losses, accuracies = train_logistic_regression(nb_epochs, learning_rate, in_dim, fea_cutter)

print('w', disc_fea5.w.item(), 'b', disc_fea5.b.item())

val_acc = evaluate(disc_fea5, val_dataset.xs[:, FOI][:, np.newaxis], val_dataset.targets)
print('disc_fea5:', val_acc)

epochs = np.arange(1, len(losses) + 1)

plt.figure(figsize=(12, 5))

# Graf 1: Průběh trénovací ztráty a validační přesnosti
plt.subplot(1, 2, 1)
plt.plot(epochs, losses, label='Training Loss')
plt.plot(epochs, accuracies, label='Validation Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Loss / Accuracy')
plt.legend()
plt.title('Training Loss and Validation Accuracy over Epochs')

# Graf 2: Histogramy trénovacích dat a pravděpodobnosti třídy 1
plt.subplot(1, 2, 2)
x_range = np.linspace(-0.5, 1.5, 100)[:, np.newaxis]
probs = disc_fea5.prob_class_1(x_range)


plt.hist(train_dataset.pos[:, FOI], bins=10, color='blue', alpha=0.4, label='Positives (Data)', density=True, histtype='stepfilled')
plt.hist(train_dataset.neg[:, FOI], bins=10, color='red', alpha=0.4, label='Negatives (Data)', density=True, histtype='stepfilled')

plt.plot(x_range, probs, label='Probability of Class 1', color='red')
plt.xlabel(f'Feature {FOI}')
plt.ylabel('Probability density')
plt.legend()
plt.title('Histograms and Probability of Class 1')

plt.tight_layout()
plt.show()

"""## Všechny vstupní příznaky
V posledním cvičení natrénujete logistickou regresi, která využije všechn sedm vstupních příznaků.
Zavolejte funkci z předchozího cvičení, opět vykreslete průběh trénovací loss a validační přesnosti.
Měli byste se dostat nad 90 % přesnosti.

Může se Vám hodit `lambda` funkce.

**(1 + 5 řádků)**
"""

nb_epochs = 100
lr = 0.01

fea_preprocessor_full = lambda x: x  # Pouze vrací vstupní data

disc_full_fea, losses, accuracies = train_logistic_regression(nb_epochs, lr, train_dataset.xs.shape[1], fea_preprocessor_full)

print('w:', disc_full_fea.w)
print('b:', disc_full_fea.b)

val_acc = evaluate(disc_full_fea, val_dataset.xs, val_dataset.targets)
print('Validation accuracy (full feature set):', val_acc)

plt.figure(figsize=(10, 5))

plt.plot(range(nb_epochs), losses, label='Training Loss')
plt.plot(range(nb_epochs), accuracies, label='Validation Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Loss / Accuracy')
plt.title('Training Loss and Validation Accuracy over Epochs (Full Feature Set)')
plt.legend()
plt.show()

"""# Závěrem
Konečně vyhodnoťte všech pět vytvořených klasifikátorů na testovacích datech.
Stačí doplnit jejich názvy a předat jim odpovídající příznaky.
Nezapomeňte, že u logistické regrese musíte zopakovat formátovací krok z `FeatureCutter`u.
"""

xs_full = test_dataset.xs
xs_foi = test_dataset.xs[:, FOI]
targets = test_dataset.targets

print('Baseline:', evaluate(baseline, xs_full, targets))
print('Generative classifier (w/o prior):', evaluate(classifier_flat_prior, xs_foi, targets))
print('Generative classifier (correct):', evaluate(classifier_full_prior, xs_foi, targets))
print('Logistic regression (Feature 5):', evaluate(disc_fea5, xs_foi[:, np.newaxis], targets))
print('Logistic regression (all features):', evaluate(disc_full_fea, xs_full, targets))

"""Blahopřejeme ke zvládnutí projektu! Nezapomeňte (1) spustit celý notebook načisto (Kernel -> Restart & Run all), (2) zkontrolovat, že všechny výpočty prošly podle očekávání, a (3) před odevzdáním pojmenovat soubory loginem vedoucího týmu.

Mimochodem, vstupní data nejsou synteticky generovaná.
Nasbírali jsme je z baseline řešení historicky prvního SUI projektu; vaše klasifikátory v tomto projektu predikují, že daný hráč vyhraje dicewars, takže by se daly použít jako heuristika pro ohodnocování listových uzlů ve stavovém prostoru hry.
Pro představu, data jsou z pozic pět kol před koncem partie pro daného hráče.
Poskytnuté příznaky popisují globální charakteristiky stavu hry jako je například poměr délky hranic předmětného hráče k ostatním hranicím.
Nejeden projekt v ročníku 2020 realizoval požadované "strojové učení" v agentovi hrajicím dicewars kopií domácí úlohy, která byla předchůdkyní tohoto projektu.
"""

